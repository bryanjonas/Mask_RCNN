{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"/lfs/jonas/maskrcnn/\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatsConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"sats\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (2, 4, 8, 16, 32)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 128\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 1\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 1\n",
    "    \n",
    "    USE_MINI_MASK = False\n",
    "    MINI_MASK_SHAPE = (128,128)\n",
    "    \n",
    "config = SatsConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    \"\"\"Load the specified image and return a [H,W,3] Numpy array.\n",
    "    \"\"\"\n",
    "    from skimage.io import imread\n",
    "\n",
    "    image = np.array(imread(image_path), dtype=float)\n",
    "        \n",
    "    bands = [4,2,1]\n",
    "            \n",
    "    image = image[:,:,bands]\n",
    "        \n",
    "    #image = (image * 255) / image.max()\n",
    "        \n",
    "    mean_std_data = np.loadtxt('image_mean_std.txt', delimiter=',')\n",
    "    mean_std_data = mean_std_data[bands,:]\n",
    "    image = preprocessing_image_ms(image, mean_std_data[:,0], mean_std_data[:,1])\n",
    "        \n",
    "    return image   \n",
    "    \n",
    "def load_orig_image(image_path):\n",
    "    \"\"\"Load the specified image (without stand.) and return a [H,W,3] Numpy array.\n",
    "    \"\"\"\n",
    "    from skimage.io import imread\n",
    "    # Load image\n",
    "    image = np.array(imread(image_path), dtype=float)\n",
    "        \n",
    "    bands = [4,2,1]\n",
    "        \n",
    "    image = image[:,:,bands]\n",
    "        \n",
    "    image = (image * 255) / image.max()\n",
    "        \n",
    "    return image   \n",
    "def preprocessing_image_ms(x, mean, std):\n",
    "    # loop over image bands\n",
    "    for idx, mean_value in enumerate(mean):\n",
    "        x[..., idx] -= mean_value\n",
    "        x[..., idx] /= std[idx]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image_path = '/data/spacenet/bldg/data/validation/MUL/'\n",
    "valid_glob = glob(valid_image_path + '*.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = random.sample(valid_glob, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_image = load_image(image_path)\n",
    "orig_image = load_orig_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_coord(geom, image):\n",
    "\n",
    "        scale_x = abs(geom[0] - image.bounds[0]) * abs(image.width / (image.bounds[0] - image.bounds[2]))\n",
    "        scale_y = abs(geom[1] - image.bounds[3]) * abs(image.height / (image.bounds[1] - image.bounds[3]))\n",
    "\n",
    "\n",
    "        return scale_x, scale_y\n",
    "def preprocessing_image_ms(x, mean, std):\n",
    "        # loop over image bands\n",
    "        for idx, mean_value in enumerate(mean):\n",
    "            x[..., idx] -= mean_value\n",
    "            x[..., idx] /= std[idx]\n",
    "        return x\n",
    "    \n",
    "def load_mask(input_path, geojson_path):\n",
    "        import cv2\n",
    "        import os\n",
    "        import json\n",
    "        import rasterio as rio\n",
    "        import numpy as np\n",
    "        import scipy.ndimage as ndi\n",
    "        \n",
    "        image_filename = os.path.split(input_path)[-1]\n",
    "        json_filename = 'buildings' + image_filename[14:-4] + '.geojson'\n",
    "        geojson_file = os.path.join(geojson_path, json_filename)\n",
    "        print(json_filename)\n",
    "        #Load JSON\n",
    "        with open(geojson_file, 'r') as f:\n",
    "            geo_json = json.load(f)\n",
    "    \n",
    "        #Open image to get scale\n",
    "        image = rio.open(input_path)\n",
    "        image_shape = image.shape\n",
    "        #Load and scale all the polygons (buildings)\n",
    "        polys = []\n",
    "\n",
    "        for feature in geo_json['features']:\n",
    "            scaled_coordSet = []\n",
    "            if feature['geometry']['type'] == 'Polygon':\n",
    "                for coordinatesSet in feature['geometry']['coordinates']:\n",
    "                    for coordinates in coordinatesSet:\n",
    "                        scale_x, scale_y = scale_coord(coordinates, image)\n",
    "                        scaled_coordSet += [[scale_x, scale_y]]\n",
    "\n",
    "        \n",
    "            if feature['geometry']['type'] == 'MultiPolygon':\n",
    "                for polygon in feature['geometry']['coordinates']:\n",
    "                    for coordinatesSet in polygon:\n",
    "                        scaled_coord = []\n",
    "                        for coordinates in coordinatesSet:\n",
    "                            scale_x, scale_y = scale_coord(coordinates, image)\n",
    "                            scaled_coord += [[scale_x, scale_y]]\n",
    "                    scaled_coord = np.array(scaled_coord)\n",
    "                scaled_coordSet += [scaled_coord]\n",
    "\n",
    "            geom_fixed = np.array(scaled_coordSet, dtype=np.int32)\n",
    "    \n",
    "            if geom_fixed.shape[0] != 0:\n",
    "                polys += [geom_fixed]\n",
    "\n",
    "        polys = np.array(polys)\n",
    "\n",
    "        mask = np.zeros(image_shape)\n",
    "        cv2.fillPoly(mask, polys, 1)\n",
    "    \n",
    "        mask = mask.reshape(mask.shape[0], mask.shape[1])\n",
    "        \n",
    "        segs, count = ndi.label(mask)\n",
    "        if count == 0:\n",
    "            maskArr = np.empty([0, 0, 0])\n",
    "            class_ids = np.empty([0], np.int32)\n",
    "        else:\n",
    "            maskArr = np.empty((segs.shape[0], segs.shape[1]))\n",
    "            class_id_list = []\n",
    "            for i in range(1, count+1):\n",
    "                intArr = (segs == i)\n",
    "                intArr.astype(int)\n",
    "                maskArr = np.dstack((maskArr, intArr))\n",
    "                class_id_list += [1]\n",
    "            maskArr = np.delete(maskArr, 0, axis=2)\n",
    "            \n",
    "            class_ids = np.array(class_id_list)\n",
    "        return maskArr, class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buildings_AOI_2_Vegas_img3701.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-bd04f0fb5c65>:61: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  polys = np.array(polys)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(650, 650, 27)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask, _ = load_mask(image_path, '/data/spacenet/bldg/data/validation/geojson/')\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = utils.extract_bboxes(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4  83  70 179]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1bc73791c0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAEyCAYAAABu5MwMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATkUlEQVR4nO3df4xV933m8ffjAY9/xMQmNoQCsXFFU9vZhnhZnNSp2w1pTdMfuNq1RFaJ6Mq7dFduk6qrbWEjbVVF0aarKtv9o84um6RFTRovok2MrCouJvWuKjVxILEdg00htRfGELCbOHYchwD+7B9z7FybGeYCd7jDt++XNDrnfM/33PvcGfz43HPvnUlVIUktumDYASRpulhwkpplwUlqlgUnqVkWnKRmWXCSmjVtBZdkVZI9SfYlWT9d9yNJk8l0vA8uyQjwd8DPAmPAV4D3VtXugd+ZJE1ius7gVgD7qurvq+oHwN3A6mm6L0ma0Kxput2FwIGe7THgpt4JSdYB6wBGGPmnlzBnmqJIatnzfPuZqrpqon3TVXCZYOxVz4WraiOwEWBO5tZNWTlNUU7PQ/U7k+5bxH1cmYcAeKaWMcatk85dlt9/ZX1P/SovMn9844IRcsEPvz1vmLWLq0e/CMD3Rt/EI6Pvoyb47qXgn3z7f3HRC984nYcjNe/+2vL/Jts3XQU3Bizu2V4EHJym+xqYU5XboLxw3Ryum/fUK9s/8VNPcev7HwbgO/sPcdl/eSfJyQ33/TrOi08vhEcsOKlf01VwXwGWJlkCPAWsAf7VNN3XwPWefb3ilTOv8W/ZVTzKVTw66W1ccO2P8sKb3wDA1ewGxl9fyW8c4YG3fH7ig67/Dv/2T//dpLf5Y5v+PUse6eshSGKaCq6qjif5deA+YAT4VFXtmo77GqRcegkvvvEinrr9J181fmIUVv7STq668Pm+b+umS+9l1SVHB5rv2S8t40C9nsX5wkBvV2rVdJ3BUVV/CfzldN3+dDj2+lF++ton2frBu4YdZUIv7rmaf2AOi7HgpH74SQZJzbLgJDXLgutx0aXP8qNX7x92DEkDMm3X4M5H19z4AP/tw/9j2DEkDYhncJKaZcGdR2Zd+SwXc3jYMaTzhgXXY8//Wc0vvP/jw44xqQ/80Ye57tLNw44hnTcsuB4jz3yXseOXDDvGpH7som/CyMiwY0jnDQuu10sv8dKEvydA0vnIgjuPfOzX/oCvfu8Dw44hnTcsOEnNsuAkNcuCk9QsP8nQY/HI/Sx/77eHHUPSgFhwPa4ceYQf/6lz80bao3WM+773eo7VyT+C//ilf0n+4cKTxp9+6ke4+MSz5yKe1AQLrkf94Ads/vAq/vT1U79V5Nil4X9/4A+47sIze9/c/S9exv9890rqO8+dtG/pcw/DSydOGv9u/fT4iu9kkfpiwfV45qW3wme//8ofljmVWW+cz9O/fgnXneF9HatZ1HPf5cSz3+n7mEXcd4b3Jv3jZMH1ePmvZF3J1AU3DP0Ur6Qf8lXUIfmRWd+m3vTGYceQmmbBDcmK0dkc/snLT+uYZ2oZz9SyaUoktcenqOeRmf4UWpppPIOT1CzP4Kawp36VF5l/0ni+NcrnNl3FLb/2aQD2PfEmfut3P/SqOd86Ac+/dBEA/2L9HzHv6vG/aP/AZ36Fx/7mn3Fg73yeqltedczFHObN+ZNXth+q3xnkw5H+UbHgzlAdO8bWz7+TL+19HQDff/5y9u+++lVzZj93jHrhRQC2/+0bueSC8RPm/ccv55kTFzP7Jd+0K02nVNWwMzAnc+umrBx2DEnnoftry86qWj7RPq/BSWqWBSepWRacpGZZcJKaZcFJapYFJ6lZFpykZllwkpo1ZcEl+VSSI0ke7Rmbm2Rbkr3d8oqefRuS7EuyJ8mt0xVckqbSzxncnwCrXjO2HtheVUuB7d02Sa4H1gA3dMfclWRkYGkl6TRMWXBV9X+Bb71meDWwqVvfBNzWM353VR2tqieAfcCKAWWVpNNyptfg5lfVIYBuOa8bXwgc6Jk31o2dJMm6JDuS7DjG0TOMIUmTG/SLDBP9vacJP81fVRuranlVLZ/N6IBjSNKZF9zhJAsAuuWRbnwMWNwzbxFw8MzjSdKZO9OC2wqs7dbXAvf0jK9JMppkCbAUePDsIkrSmZnyF14m+SzwM8CVScaA3wU+CmxOcgewH7gdoKp2JdkM7AaOA3dW1cl/wViSzoEpC66q3jvJrgl/Q2VVfQT4yNmEkqRB8JMMkpplwUlqlgUnqVkWnKRmWXCSmmXBSWqWBSepWRacpGZZcJKaZcFJapYFJ6lZFpykZllwkpplwUlqlgUnqVkWnKRmWXCSmmXBSWqWBSepWRacpGZZcJKaZcFJapYFJ6lZFpykZllwkpplwUlqlgUnqVkWnKRmWXCSmmXBSWqWBSepWRacpGZZcJKaNWXBJVmc5K+TPJZkV5IPduNzk2xLsrdbXtFzzIYk+5LsSXLrdD4ASZpMP2dwx4H/UFXXAW8H7kxyPbAe2F5VS4Ht3TbdvjXADcAq4K4kI9MRXpJOZcqCq6pDVfXVbv154DFgIbAa2NRN2wTc1q2vBu6uqqNV9QSwD1gx6OCSNJXTugaX5BrgbcCXgflVdQjGSxCY101bCBzoOWysG3vtba1LsiPJjmMcPf3kkjSFvgsuyeuAPwd+s6qeO9XUCcbqpIGqjVW1vKqWz2a03xiS1Le+Ci7JbMbL7TNV9Rfd8OEkC7r9C4Aj3fgYsLjn8EXAwcHElaT+9fMqaoBPAo9V1cd6dm0F1nbra4F7esbXJBlNsgRYCjw4uMiS1J9Zfcy5GXg/8PUkD3Vj/wn4KLA5yR3AfuB2gKralWQzsJvxV2DvrKoTA08uSVOYsuCq6m+Y+LoawMpJjvkI8JGzyCVJZ81PMkhqlgUnqVkWnKRmWXCSmmXBSWqWBSepWRacpGZZcJKaZcFJapYFJ6lZFpykZllwkpplwUlqlgUnqVkWnKRmWXCSmmXBSWqWBSepWRacpGZZcJKaZcFJapYFJ6lZFpykZllwkpplwUlqlgUnqVkWnKRmWXCSmmXBSWqWBSepWRacpGZZcJKaZcFJataUBZfkoiQPJnk4ya4kv9eNz02yLcnebnlFzzEbkuxLsifJrdP5ACRpMv2cwR0F3lVVbwWWAauSvB1YD2yvqqXA9m6bJNcDa4AbgFXAXUlGpiO8JJ3KlAVX477bbc7uvgpYDWzqxjcBt3Xrq4G7q+poVT0B7ANWDDS1JPWhr2twSUaSPAQcAbZV1ZeB+VV1CKBbzuumLwQO9Bw+1o299jbXJdmRZMcxjp7NY5CkCfVVcFV1oqqWAYuAFUnecorpmegmJrjNjVW1vKqWz2a0v7SSdBpO61XUqnoWeIDxa2uHkywA6JZHumljwOKewxYBB886qSSdpn5eRb0qyeXd+sXAu4HHga3A2m7aWuCebn0rsCbJaJIlwFLgwUEHl6SpzOpjzgJgU/dK6AXA5qq6N8nfApuT3AHsB24HqKpdSTYDu4HjwJ1VdWJ64kvS5FJ10uWxc25O5tZNWTnsGJLOQ/fXlp1VtXyifX6SQVKzLDhJzbLgJDXLgpPULAtOUrMsOEnNsuAkNcuCk9QsC05Ssyw4Sc2y4CQ1y4KT1CwLTlKzLDhJzbLgJDXLgpPULAtOUrMsOEnNsuAkNcuCk9QsC05Ssyw4Sc2y4CQ1y4KT1CwLTlKzLDhJzbLgJDXLgpPULAtOUrMsOEnNsuAkNcuCk9SsvgsuyUiSryW5t9uem2Rbkr3d8oqeuRuS7EuyJ8mt0xFckqZyOmdwHwQe69leD2yvqqXA9m6bJNcDa4AbgFXAXUlGBhNXkvrXV8ElWQT8AvCJnuHVwKZufRNwW8/43VV1tKqeAPYBKwYTV5L61+8Z3B8Cvw281DM2v6oOAXTLed34QuBAz7yxbuxVkqxLsiPJjmMcPe3gkjSVKQsuyS8CR6pqZ5+3mQnG6qSBqo1Vtbyqls9mtM+blqT+zepjzs3ALyd5D3ARMCfJp4HDSRZU1aEkC4Aj3fwxYHHP8YuAg4MMLUn9mPIMrqo2VNWiqrqG8RcPvlhV7wO2Amu7aWuBe7r1rcCaJKNJlgBLgQcHnlySptDPGdxkPgpsTnIHsB+4HaCqdiXZDOwGjgN3VtWJs04qSacpVSddHjvn5mRu3ZSVw44h6Tx0f23ZWVXLJ9rnJxkkNcuCk9QsC05Ssyw4Sc2y4CQ1y4KT1CwLTlKzLDhJzbLgJDXLgpPULAtOUrMsOEnNsuAkNcuCk9QsC05Ssyw4Sc2y4CQ1y4KT1CwLTlKzLDhJzbLgJDXLgpPULAtOUrMsOEnNsuAkNcuCk9QsC05Ssyw4Sc2y4CQ1y4KT1CwLTlKzLDhJzbLgJDWrr4JL8mSSryd5KMmObmxukm1J9nbLK3rmb0iyL8meJLdOV3hJOpXTOYP751W1rKqWd9vrge1VtRTY3m2T5HpgDXADsAq4K8nIADNLUl/O5inqamBTt74JuK1n/O6qOlpVTwD7gBVncT+SdEb6LbgC/irJziTrurH5VXUIoFvO68YXAgd6jh3rxl4lybokO5LsOMbRM0svSacwq895N1fVwSTzgG1JHj/F3EwwVicNVG0ENgLMydyT9kvS2errDK6qDnbLI8DnGH/KeTjJAoBueaSbPgYs7jl8EXBwUIElqV9TFlySS5Nc9vI68HPAo8BWYG03bS1wT7e+FViTZDTJEmAp8OCgg0vSVPp5ijof+FySl+f/WVV9IclXgM1J7gD2A7cDVNWuJJuB3cBx4M6qOjEt6SXpFFI1/MtfczK3bsrKYceQdB66v7bs7Hn72qv4SQZJzbLgJDXLgpPULAtOUrMsOEnNsuAkNcuCk9QsC05Ssyw4Sc2y4CQ1y4KT1CwLTlKzLDhJzbLgJDXLgpPULAtOUrMsOEnNsuAkNcuCk9QsC05Ssyw4Sc2y4CQ1y4KT1CwLTlKzLDhJzbLgJDXLgpPULAtOUrMsOEnNsuAkNcuCk9QsC05Ssyw4Sc3qq+CSXJ5kS5LHkzyW5B1J5ibZlmRvt7yiZ/6GJPuS7Ely6/TFl6TJ9XsG99+BL1TVjwNvBR4D1gPbq2opsL3bJsn1wBrgBmAVcFeSkUEHl6SpTFlwSeYAtwCfBKiqH1TVs8BqYFM3bRNwW7e+Gri7qo5W1RPAPmDFoINL0lT6OYO7Fnga+OMkX0vyiSSXAvOr6hBAt5zXzV8IHOg5fqwbe5Uk65LsSLLjGEfP6kFI0kT6KbhZwI3Ax6vqbcALdE9HJ5EJxuqkgaqNVbW8qpbPZrSvsJJ0OvopuDFgrKq+3G1vYbzwDidZANAtj/TMX9xz/CLg4GDiSlL/piy4qvomcCDJm7uhlcBuYCuwthtbC9zTrW8F1iQZTbIEWAo8ONDUktSHWX3O+w3gM0kuBP4e+NeMl+PmJHcA+4HbAapqV5LNjJfgceDOqjox8OSSNIVUnXR57Jybk7l1U1YOO4ak89D9tWVnVS2faJ+fZJDULAtOUrMsOEnNsuAkNcuCk9QsC05Ssyw4Sc2y4CQ1y4KT1CwLTlKzLDhJzbLgJDXLgpPULAtOUrMsOEnNmhG/Dy7J04z/rYdnhp1lClcy8zOCOQfNnIMzHRmvrqqrJtoxIwoOIMmOyX5p3UxxPmQEcw6aOQfnXGf0KaqkZllwkpo1kwpu47AD9OF8yAjmHDRzDs45zThjrsFJ0qDNpDM4SRooC05Ss4ZecElWJdmTZF+S9UPO8qkkR5I82jM2N8m2JHu75RU9+zZ0ufckufUcZVyc5K+TPJZkV5IPztCcFyV5MMnDXc7fm4k5e+57JMnXktw7U3MmeTLJ15M8lGTHDM55eZItSR7v/p2+Y2g5q2poX8AI8A3gWuBC4GHg+iHmuQW4EXi0Z+y/Auu79fXA73fr13d5R4El3eMYOQcZFwA3duuXAX/XZZlpOQO8rlufDXwZePtMy9mT97eAPwPunYk/9+6+nwSufM3YTMy5Cfg33fqFwOXDynlO/vGc4hvxDuC+nu0NwIYhZ7rmNQW3B1jQrS8A9kyUFbgPeMcQ8t4D/OxMzglcAnwVuGkm5gQWAduBd/UU3EzMOVHBzaicwBzgCboXMIedc9hPURcCB3q2x7qxmWR+VR0C6JbzuvGhZ09yDfA2xs+OZlzO7mnfQ8ARYFtVzcicwB8Cvw281DM2E3MW8FdJdiZZN0NzXgs8Dfxx95T/E0kuHVbOYRdcJhg7X963MtTsSV4H/Dnwm1X13KmmTjB2TnJW1YmqWsb4GdKKJG85xfSh5Ezyi8CRqtrZ7yETjJ2rn/vNVXUj8PPAnUluOcXcYeWcxfhlno9X1dsY/4z5qa6tT2vOYRfcGLC4Z3sRcHBIWSZzOMkCgG55pBsfWvYksxkvt89U1V/M1Jwvq6pngQeAVcy8nDcDv5zkSeBu4F1JPj0Dc1JVB7vlEeBzwIoZmHMMGOvO1gG2MF54Q8k57IL7CrA0yZIkFwJrgK1DzvRaW4G13fpaxq95vTy+JslokiXAUuDB6Q6TJMAngceq6mMzOOdVSS7v1i8G3g08PtNyVtWGqlpUVdcw/u/vi1X1vpmWM8mlSS57eR34OeDRmZazqr4JHEjy5m5oJbB7aDnPxcXRKS5KvofxVwK/AXxoyFk+CxwCjjH+f5Y7gDcwfgF6b7ec2zP/Q13uPcDPn6OM72T8FP4R4KHu6z0zMOdPAF/rcj4K/OdufEblfE3mn+GHLzLMqJyMX9t6uPva9fJ/KzMtZ3e/y4Ad3c/+88AVw8rpR7UkNWvYT1EladpYcJKaZcFJapYFJ6lZFpykZllwkpplwUlq1v8HttiZfZVs7CMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import patches\n",
    "print(bbox[2])\n",
    "y1, x1, y2, x2 = bbox[2]\n",
    "_, ax = plt.subplots(1, figsize=(5,5))\n",
    "p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n",
    "                      alpha=0.7, linestyle=\"dashed\",\n",
    "                      edgecolor='blue', facecolor='none')\n",
    "ax.add_patch(p)\n",
    "ax.imshow(mask[:,:,2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class InferenceConfig(SatsConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "stan_image, orig_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id)\n",
    "\n",
    "log(\"orig_image\", orig_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(orig_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results = model.detect([stan_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "\n",
    "#plt.imshow(r['masks'][:,:,0])\n",
    "\n",
    "visualize.display_instances(orig_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            'W ', r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def display_local(image, boxes, masks, class_ids, class_names,\n",
    "                      scores=None, title=\"\",\n",
    "                      figsize=(16, 16), ax=None,\n",
    "                      show_mask=True, show_bbox=True,\n",
    "                      colors=None, captions=None):\n",
    "    \"\"\"\n",
    "    boxes: [num_instance, (y1, x1, y2, x2, class_id)] in image coordinates.\n",
    "    masks: [height, width, num_instances]\n",
    "    class_ids: [num_instances]\n",
    "    class_names: list of class names of the dataset\n",
    "    scores: (optional) confidence scores for each box\n",
    "    title: (optional) Figure title\n",
    "    show_mask, show_bbox: To show masks and bounding boxes or not\n",
    "    figsize: (optional) the size of the image\n",
    "    colors: (optional) An array or colors to use with each object\n",
    "    captions: (optional) A list of strings to use as captions for each object\n",
    "    \"\"\"\n",
    "    # Number of instances\n",
    "    N = boxes.shape[0]\n",
    "    if not N:\n",
    "        print(\"\\n*** No instances to display *** \\n\")\n",
    "    else:\n",
    "        assert boxes.shape[0] == masks.shape[-1] == class_ids.shape[0]\n",
    "\n",
    "    # If no axis is passed, create one and automatically call show()\n",
    "    auto_show = False\n",
    "    if not ax:\n",
    "        _, ax = plt.subplots(1, figsize=figsize)\n",
    "        auto_show = True\n",
    "\n",
    "    # Generate random colors\n",
    "    colors = colors or visualize.random_colors(N)\n",
    "\n",
    "    # Show area outside image boundaries.\n",
    "    height, width = image.shape[:2]\n",
    "    ax.set_ylim(height + 10, -10)\n",
    "    ax.set_xlim(-10, width + 10)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    masked_image = image.astype(np.uint32).copy()\n",
    "    for i in range(N):\n",
    "        color = colors[i]\n",
    "\n",
    "        # Bounding box\n",
    "        #if not np.any(boxes[i]):\n",
    "            # Skip this instance. Has no bbox. Likely lost in image cropping.\n",
    "        #    continue\n",
    "        #y1, x1, y2, x2 = boxes[i]\n",
    "        #if show_bbox:\n",
    "        #    p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n",
    "        #                        alpha=0.7, linestyle=\"dashed\",\n",
    "        #                        edgecolor=color, facecolor='none')\n",
    "        #    ax.add_patch(p)\n",
    "\n",
    "        # Label\n",
    "        #if not captions:\n",
    "        #    class_id = class_ids[i]\n",
    "        #    score = scores[i] if scores is not None else None\n",
    "        #    label = class_names[class_id]\n",
    "        #    caption = \"{} {:.3f}\".format(label, score) if score else label\n",
    "        #else:\n",
    "        #    caption = captions[i]\n",
    "        #ax.text(x1, y1 + 8, caption,\n",
    "        #        color='w', size=11, backgroundcolor=\"none\")\n",
    "\n",
    "        # Mask\n",
    "        mask = masks[:, :, i]\n",
    "        if show_mask:\n",
    "            masked_image = visualize.apply_mask(masked_image, mask, color)\n",
    "\n",
    "        # Mask Polygon\n",
    "        # Pad to ensure proper polygons for masks that touch image edges.\n",
    "        padded_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
    "        padded_mask[1:-1, 1:-1] = mask\n",
    "        contours = visualize.find_contours(padded_mask, 0.5)\n",
    "        \n",
    "        for verts in contours:\n",
    "            # Subtract the padding and flip (y, x) to (x, y)\n",
    "            verts = np.fliplr(verts) - 1\n",
    "            p = visualize.Polygon(verts, facecolor=\"none\", edgecolor=color)\n",
    "            ax.add_patch(p)\n",
    "    ax.imshow(masked_image.astype(np.uint8))\n",
    "    if auto_show:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "display_local(orig_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            'W ', r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def display_centers(image, boxes, masks, class_ids, class_names,\n",
    "                      scores=None, title=\"\",\n",
    "                      figsize=(16, 16), ax=None,\n",
    "                      show_mask=True, show_bbox=True,\n",
    "                      colors=None, captions=None):\n",
    "    \"\"\"\n",
    "    boxes: [num_instance, (y1, x1, y2, x2, class_id)] in image coordinates.\n",
    "    masks: [height, width, num_instances]\n",
    "    class_ids: [num_instances]\n",
    "    class_names: list of class names of the dataset\n",
    "    scores: (optional) confidence scores for each box\n",
    "    title: (optional) Figure title\n",
    "    show_mask, show_bbox: To show masks and bounding boxes or not\n",
    "    figsize: (optional) the size of the image\n",
    "    colors: (optional) An array or colors to use with each object\n",
    "    captions: (optional) A list of strings to use as captions for each object\n",
    "    \"\"\"\n",
    "    from matplotlib import patches\n",
    "    # Number of instances\n",
    "    N = boxes.shape[0]\n",
    "    if not N:\n",
    "        print(\"\\n*** No instances to display *** \\n\")\n",
    "    else:\n",
    "        assert boxes.shape[0] == masks.shape[-1] == class_ids.shape[0]\n",
    "\n",
    "    # If no axis is passed, create one and automatically call show()\n",
    "    auto_show = False\n",
    "    if not ax:\n",
    "        _, ax = plt.subplots(1, figsize=figsize)\n",
    "        auto_show = True\n",
    "\n",
    "    # Generate random colors\n",
    "    colors = colors or visualize.random_colors(N)\n",
    "\n",
    "    # Show area outside image boundaries.\n",
    "    height, width = image.shape[:2]\n",
    "    ax.set_ylim(height + 10, -10)\n",
    "    ax.set_xlim(-10, width + 10)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    masked_image = image.astype(np.uint32).copy()\n",
    "    for i in range(N):\n",
    "        color = colors[i]\n",
    "\n",
    "        # Bounding box\n",
    "        if not np.any(boxes[i]):\n",
    "            # Skip this instance. Has no bbox. Likely lost in image cropping.\n",
    "            continue\n",
    "        else:\n",
    "            y1, x1, y2, x2 = boxes[i]\n",
    "            cen_y = y1 + ((y2 - y1)/2)\n",
    "            cen_x = x1 + ((x2 - x1)/2)\n",
    "            p = patches.Circle((cen_x, cen_y), 5)\n",
    "            ax.add_patch(p)\n",
    "\n",
    "\n",
    "    ax.imshow(masked_image.astype(np.uint8))\n",
    "    if auto_show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "display_centers(orig_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            'W ', r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
